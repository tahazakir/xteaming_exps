# config/config.yaml

attacker:
  provider: "transformers"
  model: "Qwen/Qwen2.5-72B-Instruct"
  quantization: "4bit"
  temperature: 0.3
  max_retries: 1
  max_turns: 3                  # Minimal for quick testing (increase for real runs)
  plans_file: './strategies/attack_plans.json'
  plan_revision: false          # Disable for quick testing
  run_all_strategies: false     # only require one successful strategy per behavior
  strategies_per_behavior: 1    # Minimal for quick testing (set to 10+ for real runs)
  sets_per_behavior: 1          # Must match num_sets in attack_plan_generator
  strategies_per_set: 2         # Must match num_strategies_per_set in attack_plan_generator

target:
  provider: "transformers"
  model: "meta-llama/Llama-3.1-8B-Instruct"
  temperature: 0
  max_retries: 1

textgrad:
  enabled: true
  provider: "transformers"
  model: "Qwen/Qwen2.5-72B-Instruct"
  quantization: "4bit"
  temperature: 0
  max_retries: 1
  max_turns_per_phase: 4

attack_validation:
  max_tokens_for_evaluation: 512

multithreading:
  max_workers: 1  # Set to 1-2 for local transformers (sequential generation)

attack_plan_generator:
  provider: "transformers"
  model: "Qwen/Qwen2.5-72B-Instruct"
  quantization: "4bit"
  temperature: 0.5
  max_retries: 1
  behavior_path: "./behaviors/harmbench_behaviors_text_test.csv"
  attack_plan_generation_dir: "./strategies"
  num_behaviors: 2
  num_strategies_per_set: 2   # Minimal for testing (set to 10 for real runs)
  num_sets: 1                 # Minimal for testing (set to 5 for real runs)

evaluation:
  use_gpt_judge: true
  provider: "transformers"
  model: "Qwen/Qwen2.5-72B-Instruct"
  quantization: "4bit"
  temperature: 0
  max_retries: 1
